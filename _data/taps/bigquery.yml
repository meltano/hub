name: BigQuery
type: tap
source:
  type: database
  name: Google BigQuery data warehouse
  url: https://cloud.google.com/bigquery
description: BigQuery data warehouse
singer_name: tap-bigquery
variants:
- name: anelendata
  default: true
  maintainer:
    name: Anelen
    link: https://anelen.co
  maintenance_status: Active
  repo: https://github.com/anelendata/tap-bigquery
  pip_url: tap-bigquery
  capabilities:
    - catalog
    - discover
    - state
  settings_group_validation:
    - ['streams', 'start_datetime', 'credentials_path']
  settings:
    - name: streams
      kind: array
      description: Array holding objects describing streams (tables) to extract, with `name`, `table`, `columns`, `datetime_key`, and `filters` keys. See docs for details.
    - name: credentials_path # TODO: This is not a real setting, so it wouldn't show up in the `config.json` JSON schema. We should probably make it a real setting!
      # The tap doesn't know this setting, but will read the GOOGLE_APPLICATION_CREDENTIALS env var.
      env_aliases: [GOOGLE_APPLICATION_CREDENTIALS]
      meltano_default: $MELTANO_PROJECT_ROOT/client_secrets.json
      description: Fully qualified path to `client_secrets.json` for your service account.
    - name: start_datetime
      kind: date_iso8601
      description: Determines how much historical data will be extracted. Please be aware that the larger the time period and amount of data, the longer the initial extraction can be expected to take.
    - name: end_datetime
      kind: date_iso8601
      description: Date up to when historical data will be extracted.
    - name: limit
      kind: integer
      description: Limits the number of records returned in each stream, applied as a limit in the query.
    - name: start_always_inclusive
      kind: boolean
      value: true
      description: When replicating incrementally, disable to only select records whose `datetime_key` is greater than the maximum value replicated in the last run, by excluding records whose timestamps match exactly. This could cause records to be missed that were created after the last run finished, but during the same second and with the same timestamp.
