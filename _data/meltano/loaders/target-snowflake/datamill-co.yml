name: target-snowflake
label: Snowflake
description: Snowflake database loader
namespace: target_snowflake
dialect: snowflake
target_schema: $TARGET_SNOWFLAKE_SNOWFLAKE_SCHEMA
logo_url: /assets/logos/loaders/snowflake.png
variant: datamill-co
repo: https://github.com/datamill-co/target-snowflake
pip_url: target-snowflake
settings_group_validation:
- - snowflake_account
  - snowflake_username
  - snowflake_password
  - snowflake_database
  - snowflake_warehouse
settings:
- name: snowflake_account
  label: Account
  description: |
    `ACCOUNT` might require the `region` and `cloud` platform where your account is located, in the form of: `<your_account_name>.<region_id>.<cloud>` (e.g. `xy12345.east-us-2.azure`)

    Refer to [Snowflake's documentation](https://docs.snowflake.net/manuals/user-guide/connecting.html#your-snowflake-account-name-and-url) about Accounts.
- name: snowflake_username
  label: Username
- name: snowflake_password
  label: Password
  kind: password
- name: snowflake_role
  label: Role
  description: If not specified, Snowflake will use the user's default role.
- name: snowflake_database
  label: Snowflake Database
- name: snowflake_authenticator
  label: Snowflake Authenticator
  value: snowflake
  description: |
    Specifies the authentication provider for snowflake to use. Valud options are the internal one ("snowflake"), a browser session ("externalbrowser"), or Okta ("https://<your_okta_account_name>.okta.com"). See the snowflake docs for more details.
- name: snowflake_warehouse
  label: Warehouse
- name: invalid_records_detect
  label: Invalid Records Detect
  kind: boolean
  value: true
  description: Include `false` in your config to disable crashing on invalid records
- name: invalid_records_threshold
  label: Invalid Records Threshold
  kind: integer
  value: 0
  description: Include a positive value `n` in your config to allow at most `n` invalid
    records per stream before giving up.
- name: disable_collection
  label: Disable Collection
  kind: boolean
  value: false
  description: 'Include `true` in your config to disable Singer Usage Logging: https://github.com/datamill-co/target-snowflake#usage-logging'
- name: logging_level
  label: Logging Level
  kind: options
  value: INFO
  options:
  - label: Debug
    value: DEBUG
  - label: Info
    value: INFO
  - label: Warning
    value: WARNING
  - label: Error
    value: ERROR
  - label: Critical
    value: CRITICAL
  description: The level for logging. Set to `DEBUG` to get things like queries executed, timing of those queries, etc. See [Python's Logger Levels](https://docs.python.org/3/library/logging.html#levels) for information about valid values.
- name: persist_empty_tables
  label: Persist Empty Tables
  kind: boolean
  value: false
  description: Whether the Target should create tables which have no records present
    in Remote.
- name: snowflake_schema
  aliases: [schema]
  label: Snowflake Schema
  value: $MELTANO_EXTRACT__LOAD_SCHEMA
  value_processor: upcase_string
  description: |
    Note `$MELTANO_EXTRACT__LOAD_SCHEMA` [will expand to](https://docs.meltano.com/configuration.html#expansion-in-setting-values) the value of the [`load_schema` extra](https://docs.meltano.com/concepts/plugins#load-schema-extra) for the extractor used in the pipeline, which defaults to the extractor's namespace, e.g. `tap_gitlab` for [`tap-gitlab`](/extractors/gitlab). Values are automatically converted to uppercase before they're passed on to the plugin, so `tap_gitlab` becomes `TAP_GITLAB`.
- name: state_support
  label: State Support
  kind: boolean
  value: true
  description: Whether the Target should emit `STATE` messages to stdout for further
    consumption. In this mode, which is on by default, STATE messages are buffered
    in memory until all the records that occurred before them are flushed according
    to the batch flushing schedule the target is configured with.
- name: target_s3.bucket
  label: Target S3 Bucket
  description: When included, use S3 to stage files. Bucket where staging files should
    be uploaded to.
- name: target_s3.key_prefix
  label: Target S3 Key Prefix
  description: Prefix for staging file uploads to allow for better delineation of
    tmp files
- name: target_s3.aws_access_key_id
  label: Target S3 AWS Access Key ID
  kind: password
- name: target_s3.aws_secret_access_key
  label: Target S3 AWS Secret Access Key
  kind: password
domain_url: https://www.snowflake.com/
maintenance_status: inactive
keywords:
- database
prereq: |
  #### Dependencies

  `target-snowflake` [requires](https://www.psycopg.org/docs/install.html#runtime-requirements) the
  [`libpq` library](https://www.postgresql.org/docs/current/libpq.html) to be available on your system.
  If you've installed PostgreSQL, you should already have it, but you can also install it by itself using the
  [`libpq-dev` package](https://pkgs.org/download/libpq-dev) on Ubuntu/Debian or the
  [`libpq` Homebrew formula](https://formulae.brew.sh/formula/libpq) on macOS.
usage: |
  ## Troubleshooting

  ### Error: `pg_config executable not found` or `libpq-fe.h: No such file or directory`

  This error message indicates that the [`libpq`](https://www.postgresql.org/docs/current/libpq.html) dependency is missing.

  To resolve this, refer to the ["Dependencies" section](#dependencies) above.
