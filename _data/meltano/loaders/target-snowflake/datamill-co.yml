description: Snowflake database loader
dialect: snowflake
domain_url: https://www.snowflake.com/
keywords:
- database
label: Snowflake
logo_url: /assets/logos/loaders/snowflake.png
maintenance_status: inactive
name: target-snowflake
namespace: target_snowflake
pip_url: target-snowflake
prereq: |
  #### Dependencies

  `target-snowflake` [requires](https://www.psycopg.org/docs/install.html#runtime-requirements) the
  [`libpq` library](https://www.postgresql.org/docs/current/libpq.html) to be available on your system.
  If you've installed PostgreSQL, you should already have it, but you can also install it by itself using the
  [`libpq-dev` package](https://pkgs.org/download/libpq-dev) on Ubuntu/Debian or the
  [`libpq` Homebrew formula](https://formulae.brew.sh/formula/libpq) on macOS.
quality: silver
repo: https://github.com/datamill-co/target-snowflake
settings:
- description: |
    `ACCOUNT` might require the `region` and `cloud` platform where your account is located, in the form of: `<your_account_name>.<region_id>.<cloud>` (e.g. `xy12345.east-us-2.azure`)

    Refer to [Snowflake's documentation](https://docs.snowflake.net/manuals/user-guide/connecting.html#your-snowflake-account-name-and-url) about Accounts.
  label: Account
  name: snowflake_account
- label: Username
  name: snowflake_username
- kind: password
  label: Password
  name: snowflake_password
- description: If not specified, Snowflake will use the user's default role.
  label: Role
  name: snowflake_role
- label: Snowflake Database
  name: snowflake_database
- description: |
    Specifies the authentication provider for snowflake to use. Valud options are the internal one ("snowflake"), a browser session ("externalbrowser"), or Okta ("https://<your_okta_account_name>.okta.com"). See the snowflake docs for more details.
  label: Snowflake Authenticator
  name: snowflake_authenticator
  value: snowflake
- label: Warehouse
  name: snowflake_warehouse
- description: Include `false` in your config to disable crashing on invalid records
  kind: boolean
  label: Invalid Records Detect
  name: invalid_records_detect
  value: true
- description: Include a positive value `n` in your config to allow at most `n` invalid
    records per stream before giving up.
  kind: integer
  label: Invalid Records Threshold
  name: invalid_records_threshold
  value: 0
- description: 'Include `true` in your config to disable Singer Usage Logging: https://github.com/datamill-co/target-snowflake#usage-logging'
  kind: boolean
  label: Disable Collection
  name: disable_collection
  value: false
- description: The level for logging. Set to `DEBUG` to get things like queries executed,
    timing of those queries, etc. See [Python's Logger Levels](https://docs.python.org/3/library/logging.html#levels)
    for information about valid values.
  kind: options
  label: Logging Level
  name: logging_level
  options:
  - label: Debug
    value: DEBUG
  - label: Info
    value: INFO
  - label: Warning
    value: WARNING
  - label: Error
    value: ERROR
  - label: Critical
    value: CRITICAL
  value: INFO
- description: Whether the Target should create tables which have no records present
    in Remote.
  kind: boolean
  label: Persist Empty Tables
  name: persist_empty_tables
  value: false
- aliases:
  - schema
  description: |
    Note `$MELTANO_EXTRACT__LOAD_SCHEMA` [will expand to](https://docs.meltano.com/guide/configuration.html#expansion-in-setting-values) the value of the [`load_schema` extra](https://docs.meltano.com/concepts/plugins#load-schema-extra) for the extractor used in the pipeline, which defaults to the extractor's namespace, e.g. `tap_gitlab` for [`tap-gitlab`](/extractors/gitlab). Values are automatically converted to uppercase before they're passed on to the plugin, so `tap_gitlab` becomes `TAP_GITLAB`.
  label: Snowflake Schema
  name: snowflake_schema
  value: $MELTANO_EXTRACT__LOAD_SCHEMA
  value_processor: upcase_string
- description: Whether the Target should emit `STATE` messages to stdout for further
    consumption. In this mode, which is on by default, STATE messages are buffered
    in memory until all the records that occurred before them are flushed according
    to the batch flushing schedule the target is configured with.
  kind: boolean
  label: State Support
  name: state_support
  value: true
- description: When included, use S3 to stage files. Bucket where staging files should
    be uploaded to.
  label: Target S3 Bucket
  name: target_s3.bucket
- description: Prefix for staging file uploads to allow for better delineation of
    tmp files
  label: Target S3 Key Prefix
  name: target_s3.key_prefix
- kind: password
  label: Target S3 AWS Access Key ID
  name: target_s3.aws_access_key_id
- kind: password
  label: Target S3 AWS Secret Access Key
  name: target_s3.aws_secret_access_key
settings_group_validation:
- - snowflake_account
  - snowflake_username
  - snowflake_password
  - snowflake_database
  - snowflake_warehouse
target_schema: $TARGET_SNOWFLAKE_SNOWFLAKE_SCHEMA
usage: |
  ## Troubleshooting

  ### Error: `pg_config executable not found` or `libpq-fe.h: No such file or directory`

  This error message indicates that the [`libpq`](https://www.postgresql.org/docs/current/libpq.html) dependency is missing.

  To resolve this, refer to the ["Dependencies" section](#dependencies) above.
variant: datamill-co
