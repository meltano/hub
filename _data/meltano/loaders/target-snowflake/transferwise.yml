description: Snowflake database loader
dialect: snowflake
domain_url: https://www.snowflake.com/
keywords:
- database
label: Snowflake
logo_url: /assets/logos/loaders/snowflake.png
maintenance_status: active
name: target-snowflake
namespace: target_snowflake
pip_url: pipelinewise-target-snowflake
prereq: |
  #### Dependencies

  A Snowflake `FILE FORMAT` object must exist prior to execution and is a required config input. You can use the sample SQL provided below:

  ```sql
  CREATE FILE FORMAT {database}.{schema}.{file_format_name}
  TYPE = 'CSV' ESCAPE='\\' FIELD_OPTIONALLY_ENCLOSED_BY='"';
  ```

  See the [documentation](https://github.com/transferwise/pipelinewise-target-snowflake#pre-requirements) for more details on other optional objects and how to create them.
quality: silver
repo: https://github.com/transferwise/pipelinewise-target-snowflake
settings:
- description: Snowflake account name (i.e. `rtXXXXX.eu-central-1`)
  label: Account
  name: account
  placeholder: E.g. rtXXXXX.eu-central-1
- aliases:
  - database
  description: Snowflake Database name
  label: Database Name
  name: dbname
- aliases:
  - username
  description: Snowflake User
  label: User
  name: user
- description: Snowflake Password
  kind: password
  label: Password
  name: password
- description: Snowflake virtual warehouse name
  label: Warehouse
  name: warehouse
- description: The Snowflake file format object name which needs to be manually created
    as part of the [requirements](#requirements) section of the docs. Has to be the
    fully qualified name including the schema. Refer to the [Snowflake docs](https://github.com/transferwise/pipelinewise-target-snowflake#pre-requirements)
    for more details.
  label: File Format
  name: file_format
- description: Snowflake role to use. If not defined then the user's default role
    will be used.
  label: Role
  name: role
- description: S3 Access Key Id. If not provided, `AWS_ACCESS_KEY_ID` environment
    variable or IAM role will be used
  kind: password
  label: AWS Access Key ID
  name: aws_access_key_id
- description: S3 Secret Access Key. If not provided, `AWS_SECRET_ACCESS_KEY` environment
    variable or IAM role will be used
  kind: password
  label: AWS Secret Access Key
  name: aws_secret_access_key
- description: AWS Session token. If not provided, `AWS_SESSION_TOKEN` environment
    variable will be used
  kind: password
  label: AWS Session Token
  name: aws_session_token
- description: AWS profile name for profile based authentication. If not provided,
    `AWS_PROFILE` environment variable will be used.
  label: AWS Profile
  name: aws_profile
- aliases:
  - schema
  description: |
    Note `$MELTANO_EXTRACT__LOAD_SCHEMA` [will expand to](https://docs.meltano.com/guide/configuration.html#expansion-in-setting-values) the value of the [`load_schema` extra](https://docs.meltano.com/concepts/plugins#load-schema-extra) for the extractor used in the pipeline, which defaults to the extractor's namespace, e.g. `tap_gitlab` for [`tap-gitlab`](/extractors/gitlab). Values are automatically converted to uppercase before they're passed on to the plugin, so `tap_gitlab` becomes `TAP_GITLAB`.

    Name of the schema where the tables will be created, without database
    prefix. If `schema_mapping` is not defined then every stream sent by the tap is
    loaded into this schema.
  label: Default Target Schema
  name: default_target_schema
  value: $MELTANO_EXTRACT__LOAD_SCHEMA
  value_processor: upcase_string
- description: S3 Bucket name
  label: S3 Bucket
  name: s3_bucket
- description: A static prefix before the generated S3 key names. Using prefixes you
    can upload files into specific directories in the S3 bucket.
  label: S3 Key Prefix
  name: s3_key_prefix
- description: The complete URL to use for the constructed client. This is allowing
    to use non-native s3 account.
  label: S3 Endpoint URL
  name: s3_endpoint_url
- description: Default region when creating new connections
  label: S3 Region Name
  name: s3_region_name
- description: S3 ACL name to set on the uploaded files
  label: S3 ACL
  name: s3_acl
- description: Named external stage name created at pre-requirements section. Has
    to be a fully qualified name including the schema name
  label: Stage
  name: stage
- description: Maximum number of rows in each batch. At the end of each batch, the
    rows in the batch are loaded into Snowflake.
  kind: integer
  label: Batch Size Rows
  name: batch_size_rows
  value: 100000
- description: Maximum time to wait for batch to reach batch_size_rows.
  kind: integer
  label: Batch Wait Limit Seconds
  name: batch_wait_limit_seconds
- description: 'Flush and load every stream into Snowflake when one batch is full.
    Warning: This may trigger the COPY command to use files with low number of records,
    and may cause performance problems.'
  kind: boolean
  label: Flush All Streams
  name: flush_all_streams
  value: false
- description: The number of threads used to flush tables. 0 will create a thread
    for each stream, up to parallelism_max. -1 will create a thread for each CPU core.
    Any other positive number will create that number of threads, up to parallelism_max.
  kind: integer
  label: Parallelism
  name: parallelism
  value: 0
- description: Max number of parallel threads to use when flushing tables.
  kind: integer
  label: Parallelism Max
  name: parallelism_max
  value: 16
- description: Grant USAGE privilege on newly created schemas and grant SELECT privilege
    on newly created tables to a specific role or a list of roles. If `schema_mapping`
    is not defined then every stream sent by the tap is granted accordingly.
  label: Default Target Schema Select Permission
  name: default_target_schema_select_permission
- description: |
    Useful if you want to load multiple streams from one tap to multiple Snowflake schemas.

    If the tap sends the `stream_id` in `<schema_name>-<table_name>` format then this option overwrites the `default_target_schema` value.

    Note, that using `schema_mapping` you can overwrite the `default_target_schema_select_permission` value to grant SELECT permissions to different groups per schemas or optionally you can create indices automatically for the replicated tables.
  kind: object
  label: Schema Mapping
  name: schema_mapping
- description: By default the connector caches the available table structures in Snowflake
    at startup. In this way it doesn't need to run additional queries when ingesting
    data to check if altering the target tables is required. With `disable_table_cache`
    option you can turn off this caching. You will always see the most recent table
    structures but will cause an extra query runtime.
  kind: boolean
  label: Disable Table Cache
  name: disable_table_cache
  value: false
- description: When this is defined, Client-Side Encryption is enabled. The data in
    S3 will be encrypted, No third parties, including Amazon AWS and any ISPs, can
    see data in the clear. Snowflake COPY command will decrypt the data once it's
    in Snowflake. The master key must be 256-bit length and must be encoded as base64
    string.
  kind: password
  label: Client Side Encryption Master Key
  name: client_side_encryption_master_key
- description: Required when `client_side_encryption_master_key` is defined. The name
    of the encrypted stage object in Snowflake that created separately and using the
    same encryption master key.
  label: Client Side Encryption Stage Object
  name: client_side_encryption_stage_object
- description: Metadata columns add extra row level information about data ingestions,
    (i.e. when was the row read in source, when was inserted or deleted in snowflake
    etc.) Metadata columns are creating automatically by adding extra columns to the
    tables with a column prefix `_SDC_`. The column names are following the stitch
    naming conventions documented at https://www.stitchdata.com/docs/data-structure/integration-schemas#sdc-columns.
    Enabling metadata columns will flag the deleted rows by setting the `_SDC_DELETED_AT`
    metadata column. Without the `add_metadata_columns` option the deleted rows from
    singer taps will not be recongisable in Snowflake.
  kind: boolean
  label: Add Metadata Columns
  name: add_metadata_columns
  value: false
- description: When `hard_delete` option is true then DELETE SQL commands will be
    performed in Snowflake to delete rows in tables. It's achieved by continuously
    checking the `_SDC_DELETED_AT` metadata column sent by the singer tap. Due to
    deleting rows requires metadata columns, `hard_delete` option automatically enables
    the `add_metadata_columns` option as well.
  kind: boolean
  label: Hard Delete
  name: hard_delete
  value: false
- description: Object type RECORD items from taps can be loaded into VARIANT columns
    as JSON (default) or we can flatten the schema by creating columns automatically.
    When value is 0 (default) then flattening functionality is turned off.
  kind: integer
  label: Data Flattening Max Level
  name: data_flattening_max_level
  value: 0
- description: Log based and Incremental replications on tables with no Primary Key
    cause duplicates when merging UPDATE events. When set to true, stop loading data
    if no Primary Key is defined.
  kind: boolean
  label: Primary Key Required
  name: primary_key_required
  value: true
- description: Validate every single record message to the corresponding JSON schema.
    This option is disabled by default and invalid RECORD messages will fail only
    at load time by Snowflake. Enabling this option will detect invalid records earlier
    but could cause performance degradation.
  kind: boolean
  label: Validate Records
  name: validate_records
  value: false
- description: '(Default: platform-dependent) Directory of temporary CSV files with
    RECORD messages.'
  label: Temporary Directory
  name: temp_dir
- description: Generate uncompressed CSV files when loading to Snowflake. Normally,
    by default GZIP compressed files are generated.
  kind: boolean
  label: No Compression
  name: no_compression
  value: false
- description: |
    Optional string to tag executed queries in Snowflake. Replaces tokens

    `schema` and `table` with the appropriate values. The tags are displayed in the
    output of the Snowflake `QUERY_HISTORY`, `QUERY_HISTORY_BY_*` functions.
  label: Query Tag
  name: query_tag
- description: |
    When enabled, the files loaded to Snowflake will also be stored in
    `archive_load_files_s3_bucket` under the key /{archive_load_files_s3_prefix}/{schema_name}/{table_name}/.

    All archived files will have tap, schema, table and archived-by as S3 metadata keys.

    When incremental replication is used, the archived files will also have
    the following S3 metadata keys - incremental-key, incremental-key-min and incremental-key-max.
  kind: boolean
  label: Archive Load Files
  name: archive_load_files
  value: false
- description: When `archive_load_files` is enabled, the archived files will be placed
    in the archive S3 bucket under this prefix.
  label: Archive Load Files S3 Prefix
  name: archive_load_files_s3_prefix
- description: When archive_load_files is enabled, the archived files will be placed
    in this bucket.
  label: Archive Load Files S3 Bucket
  name: archive_load_files_s3_bucket
settings_group_validation:
- - account
  - dbname
  - user
  - password
  - warehouse
  - file_format
  - default_target_schema
target_schema: $TARGET_SNOWFLAKE_DEFAULT_TARGET_SCHEMA
usage: |
  # Advanced Topics

  ## How Schema Changes Are Handled

  See the [pipelinewise](https://transferwise.github.io/pipelinewise/user_guide/schema_changes.html#schema-changes) for the full documentation.
  Here are some of the important details:

  ### 1. New Column Added

  Target connectors add the new column to the destination table with the same name using a compatible data type.

  ### 2. Column Dropped

  Target connectors DO NOT drop columns. Old column remains in the table in case you need to do historical analysis on the column.

  ### 3. Column Data Type Changed

  Target connectors version the columns. See the [versioning columns docs](https://transferwise.github.io/pipelinewise/user_guide/schema_changes.html#versioning-columns) for syntax.
variant: transferwise
