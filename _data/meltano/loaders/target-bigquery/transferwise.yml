capabilities:
- schema-flattening
- hard-delete
- soft-delete
description: For loading data into BigQuery. This repo was transferred to the jmriego
  namespace.
domain_url: https://cloud.google.com/bigquery/
hidden: true
keywords:
- google
- bigquery
label: Google BigQuery
logo_url: /assets/logos/loaders/bigquery.png
maintenance_status: inactive
name: target-bigquery
namespace: target_bigquery
pip_url: pipelinewise-target-bigquery
quality: silver
repo: https://github.com/transferwise/pipelinewise-target-bigquery
settings:
- description: |
    Fully qualified path to `client_secrets.json` for your service account.

    See the ["Activate the Google BigQuery API" section of the repository's README](https://github.com/adswerve/target-bigquery#step-1-activate-the-google-bigquery-api) and <https://cloud.google.com/docs/authentication/production>.

    By default, this file is expected to be at the root of your project directory.
  env: GOOGLE_APPLICATION_CREDENTIALS
  label: Credentials Path
  name: credentials_path
  value: $MELTANO_PROJECT_ROOT/client_secrets.json
- description: BigQuery dataset
  label: Dataset Id
  name: dataset_id
- description: BigQuery project
  label: Project Id
  name: project_id
- description: Region where BigQuery stores your dataset
  label: Location
  name: location
  value: US
- description: Maximum number of rows in each batch. At the end of each batch, the
    rows in the batch are loaded into BigQuery.
  kind: integer
  label: Batch Size Rows
  name: batch_size_rows
  value: 100000
- description: Flush and load every stream into BigQuery when one batch is full. Warning
    - This may trigger transfer of data with low number of records, and may cause
    performance problems.
  kind: boolean
  label: Flush All Streams
  name: flush_all_streams
  value: false
- description: The number of threads used to flush tables. 0 will create a thread
    for each stream, up to parallelism_max. -1 will create a thread for each CPU core.
    Any other positive number will create that number of threads, up to parallelism_max.
  kind: integer
  label: Parallelism
  name: parallelism
  value: 0
- description: Max number of parallel threads to use when flushing tables.
  kind: integer
  label: Max Parallelism
  name: max_parallelism
  value: 16
- description: Name of the schema where the tables will be created. If schema_mapping
    is not defined then every stream sent by the tap is loaded into this schema.
  label: Default Target Schema
  name: default_target_schema
- description: Grant USAGE privilege on newly created schemas and grant SELECT privilege
    on newly created
  label: Default Target Schema Select Permission
  name: default_target_schema_select_permission
- description: (Experimental) Useful if you want to load multiple streams from one
    tap to multiple BigQuery schemas. If the tap sends the stream_id in <schema_name>-<table_name>
    format then this option overwrites the default_target_schema value. Note, that
    using schema_mapping you can overwrite the default_target_schema_select_permission
    value to grant SELECT permissions to different groups per schemas or optionally
    you can create indices automatically for the replicated tables.
  kind: object
  label: Schema Mapping
  name: schema_mapping
- description: Metadata columns add extra row level information about data ingestions,
    (i.e. when was the row read in source, when was inserted or deleted in bigquery
    etc.) Metadata columns are creating automatically by adding extra columns to the
    tables with a column prefix _sdc_. The column names are following the stitch naming
    conventions documented at https://www.stitchdata.com/docs/data-structure/integration-schemas#sdc-columns.
    Enabling metadata columns will flag the deleted rows by setting the _sdc_deleted_at
    metadata column. Without the add_metadata_columns option the deleted rows from
    singer taps will not be recognisable in BigQuery.
  kind: boolean
  label: Add Metadata Columns
  name: add_metadata_columns
  value: false
- description: When hard_delete option is true then DELETE SQL commands will be performed
    in BigQuery to delete rows in tables. It's achieved by continuously checking the
    _sdc_deleted_at metadata column sent by the singer tap. Due to deleting rows requires
    metadata columns, hard_delete option automatically enables the add_metadata_columns
    option as well.
  kind: boolean
  label: Hard Delete
  name: hard_delete
  value: false
- description: Object type RECORD items from taps can be loaded into VARIANT columns
    as JSON (default) or we can flatten the schema by creating columns automatically.
    When value is 0 (default) then flattening functionality is turned off.
  kind: integer
  label: Data Flattening Max Level
  name: data_flattening_max_level
  value: 0
- description: Log based and Incremental replications on tables with no Primary Key
    cause duplicates when merging UPDATE events. When set to true, stop loading data
    if no Primary Key is defined.
  kind: boolean
  label: Primary Key Required
  name: primary_key_required
  value: true
- description: Validate every single record message to the corresponding JSON schema.
    This option is disabled by default and invalid RECORD messages will fail only
    at load time by BigQuery. Enabling this option will detect invalid records earlier
    but could cause performance degradation.
  kind: boolean
  label: Validate Records
  name: validate_records
  value: false
- description: Name of the schema where the temporary tables will be created. Will
    default to the same schema as the target tables.
  label: Temp Schema
  name: temp_schema
settings_group_validation:
- - dataset_id
  - project_id
variant: transferwise
