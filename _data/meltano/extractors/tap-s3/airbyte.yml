capabilities:
- catalog
- state
- discover
- about
- stream-maps
- schema-flattening
description: AWS S3
domain_url: https://aws.amazon.com/s3/
executable: tap-airbyte
keywords:
- aws
- airbyte_protocol
label: S3
logo_url: /assets/logos/extractors/s3-csv.png
maintenance_status: beta
name: tap-s3
namespace: tap_airbyte
next_steps: ''
pip_url: git+https://github.com/MeltanoLabs/tap-airbyte-wrapper.git
quality: silver
repo: https://github.com/airbytehq/airbyte/tree/master/airbyte-integrations/connectors/source-s3
settings:
- description: Airbyte image to run
  kind: string
  label: Airbyte Spec Image
  name: airbyte_spec.image
  value: airbyte/source-s3
- description: Airbyte image tag
  kind: string
  label: Airbyte Spec Tag
  name: airbyte_spec.tag
  value: latest
- description: Docker mounts to make available to the Airbyte container. Expects a
    list of maps containing source, target, and type as is documented in the docker
    --mount documentation
  kind: array
  label: Docker Mounts
  name: docker_mounts
- description: The name of the stream you would like this source to output. Can contain
    letters, numbers, or underscores.
  kind: string
  label: Airbyte Config Dataset
  name: airbyte_config.dataset
- description: A regular expression which tells the connector which files to replicate.
    All files which match this pattern will be replicated. Use | to separate multiple
    patterns. See <a href="https://facelessuser.github.io/wcmatch/glob/" target="_blank">this
    page</a> to understand pattern syntax (GLOBSTAR and SPLIT flags are enabled).
    Use pattern <strong>**</strong> to pick up all files.
  kind: string
  label: Airbyte Config Path Pattern
  name: airbyte_config.path_pattern
- description: csv, parquet, avro, jsonl
  kind: string
  label: Airbyte Config Format Filetype
  name: airbyte_config.format.filetype
- description: The character delimiting individual cells in the CSV data. This may
    only be a 1-character string. For tab-delimited data enter '\t'.
  kind: string
  label: Airbyte Config Format Delimiter
  name: airbyte_config.format.delimiter
- description: Configures whether a schema for the source should be inferred from
    the current data or not. If set to false and a custom schema is set, then the
    manually enforced schema is used. If a schema is not manually set, and this is
    set to false, then all fields will be read as strings
  kind: boolean
  label: Airbyte Config Format Infer Datatypes
  name: airbyte_config.format.infer_datatypes
- description: The character used for quoting CSV values. To disallow quoting, make
    this field blank.
  kind: string
  label: Airbyte Config Format Quote Char
  name: airbyte_config.format.quote_char
- description: The character used for escaping special characters. To disallow escaping,
    leave this field blank.
  kind: string
  label: Airbyte Config Format Escape Char
  name: airbyte_config.format.escape_char
- description: The character encoding of the CSV data. Leave blank to default to <strong>UTF8</strong>.
    See <a href="https://docs.python.org/3/library/codecs.html#standard-encodings"
    target="_blank">list of python encodings</a> for allowable options.
  kind: string
  label: Airbyte Config Format Encoding
  name: airbyte_config.format.encoding
- description: Whether two quotes in a quoted CSV value denote a single quote in the
    data.
  kind: boolean
  label: Airbyte Config Format Double Quote
  name: airbyte_config.format.double_quote
- description: Whether newline characters are allowed in CSV values. Turning this
    on may affect performance. Leave blank to default to False., Whether newline characters
    are allowed in JSON values. Turning this on may affect performance. Leave blank
    to default to False.
  kind: boolean
  label: Airbyte Config Format Newlines In Values
  name: airbyte_config.format.newlines_in_values
- description: Optionally add a valid JSON string here to provide additional options
    to the csv reader. Mappings must correspond to options <a href="https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html#pyarrow.csv.ConvertOptions"
    target="_blank">detailed here</a>. 'column_types' is used internally to handle
    schema so overriding that would likely cause problems.
  kind: string
  label: Airbyte Config Format Additional Reader Options
  name: airbyte_config.format.additional_reader_options
- description: Optionally add a valid JSON string here to provide additional <a href="https://arrow.apache.org/docs/python/generated/pyarrow.csv.ReadOptions.html#pyarrow.csv.ReadOptions"
    target="_blank">Pyarrow ReadOptions</a>. Specify 'column_names' here if your CSV
    doesn't have header, or if you want to use custom column names. 'block_size' and
    'encoding' are already used above, specify them again here will override the values
    above.
  kind: string
  label: Airbyte Config Format Advanced Options
  name: airbyte_config.format.advanced_options
- description: The chunk size in bytes to process at a time in memory from each file.
    If your data is particularly wide and failing during schema detection, increasing
    this should solve it. Beware of raising this too high as you could hit OOM errors.,
    The chunk size in bytes to process at a time in memory from each file. If your
    data is particularly wide and failing during schema detection, increasing this
    should solve it. Beware of raising this too high as you could hit OOM errors.
  kind: integer
  label: Airbyte Config Format Block Size
  name: airbyte_config.format.block_size
- description: If you only want to sync a subset of the columns from the file(s),
    add the columns you want here as a comma-delimited list. Leave it empty to sync
    all columns.
  kind: array
  label: Airbyte Config Format Columns
  name: airbyte_config.format.columns
- description: Maximum number of records per batch read from the input files. Batches
    may be smaller if there arenâ€™t enough rows in the file. This option can help avoid
    out-of-memory errors if your data is particularly wide.
  kind: integer
  label: Airbyte Config Format Batch Size
  name: airbyte_config.format.batch_size
- description: Perform read buffering when deserializing individual column chunks.
    By default every group column will be loaded fully to memory. This option can
    help avoid out-of-memory errors if your data is particularly wide.
  kind: integer
  label: Airbyte Config Format Buffer Size
  name: airbyte_config.format.buffer_size
- description: How JSON fields outside of explicit_schema (if given) are treated.
    Check <a href="https://arrow.apache.org/docs/python/generated/pyarrow.json.ParseOptions.html"
    target="_blank">PyArrow documentation</a> for details
  kind: string
  label: Airbyte Config Format Unexpected Field Behavior
  name: airbyte_config.format.unexpected_field_behavior
- description: 'Optionally provide a schema to enforce, as a valid JSON string. Ensure
    this is a mapping of <strong>{ "column" : "type" }</strong>, where types are valid
    <a href="https://json-schema.org/understanding-json-schema/reference/type.html"
    target="_blank">JSON Schema datatypes</a>. Leave as {} to auto-infer the schema.'
  kind: string
  label: Airbyte Config Schema
  name: airbyte_config.schema
- description: Name of the S3 bucket where the file(s) exist.
  kind: password
  label: Airbyte Config Provider Bucket
  name: airbyte_config.provider.bucket
- description: In order to access private Buckets stored on AWS S3, this connector
    requires credentials with the proper permissions. If accessing publicly available
    data, this field is not necessary.
  kind: password
  label: Airbyte Config Provider AWS Access Key Id
  name: airbyte_config.provider.aws_access_key_id
- description: In order to access private Buckets stored on AWS S3, this connector
    requires credentials with the proper permissions. If accessing publicly available
    data, this field is not necessary.
  kind: password
  label: Airbyte Config Provider AWS Secret Access Key
  name: airbyte_config.provider.aws_secret_access_key
- description: By providing a path-like prefix (e.g. myFolder/thisTable/) under which
    all the relevant files sit, we can optimize finding these in S3. This is optional
    but recommended if your bucket contains many folders/files which you don't need
    to replicate.
  kind: password
  label: Airbyte Config Provider Path Prefix
  name: airbyte_config.provider.path_prefix
- description: Endpoint to an S3 compatible service. Leave empty to use AWS.
  kind: password
  label: Airbyte Config Provider Endpoint
  name: airbyte_config.provider.endpoint
- description: Config object for stream maps capability. For more information check
    out [Stream Maps](https://sdk.meltano.com/en/latest/stream_maps.html).
  kind: object
  label: Stream Maps
  name: stream_maps
- description: User-defined config values to be used within map expressions.
  kind: object
  label: Stream Map Config
  name: stream_map_config
- description: "'True' to enable schema flattening and automatically expand nested\
    \ properties."
  kind: boolean
  label: Flattening Enabled
  name: flattening_enabled
- description: The max depth to flatten schemas.
  kind: integer
  label: Flattening Max Depth
  name: flattening_max_depth
settings_group_validation:
- - connector_config.provider.bucket
  - connector_config.path_pattern
  - airbyte_spec.image
  - connector_config.dataset
settings_preamble: ''
usage: ''
variant: airbyte
