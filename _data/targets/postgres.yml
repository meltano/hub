name: postgres
label: PostgreSQL
type: target
description: For loading data into PostgreSQL
singer_name: target-postgres
variants:
- name: datamill-co
  maintenance_status: active
  default: false
  repo: https://github.com/datamill-co/target-postgres
  pip_url: singer-target-postgres
  dependencies:
  - '`target-postgres` [requires](https://www.psycopg.org/docs/install.html#runtime-requirements)
    the [`libpq` library](https://www.postgresql.org/docs/current/libpq.html) to be
    available on your system. If you''ve installed PostgreSQL, you should already
    have it, but you can also install it by itself using the [`libpq-dev` package](https://pkgs.org/download/libpq-dev)
    on Ubuntu/Debian or the [`libpq` Homebrew formula](https://formulae.brew.sh/formula/libpq)
    on macOS.'
  settings_group_validation:
  - - postgres_host
    - postgres_port
    - postgres_database
    - postgres_username
    - postgres_password
    - postgres_schema
  settings:
  - name: postgres_host
    value: localhost
  - name: postgres_port
    kind: integer
    value: 5432
  - name: postgres_database
  - name: postgres_username
  - name: postgres_password
    kind: password
  - name: postgres_schema
    schema_id: true
  - name: postgres_sslmode
    value: prefer
    description: 'Refer to the libpq docs for more information about SSL: https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-PARAMKEYWORDS'
  - name: postgres_sslcert
    value: ~/.postgresql/postgresql.crt
    description: Only used if a SSL request w/ a client certificate is being made
  - name: postgres_sslkey
    value: ~/.postgresql/postgresql.key
    description: Only used if a SSL request w/ a client certificate is being made
  - name: postgres_sslrootcert
    value: ~/.postgresql/root.crt
    description: Used for authentication of a server SSL certificate
  - name: postgres_sslcrl
    value: ~/.postgresql/root.crl
    description: Used for authentication of a server SSL certificate
  - name: invalid_records_detect
    kind: boolean
    value: true
    description: Include `false` in your config to disable `target-postgres` from
      crashing on invalid records
  - name: invalid_records_threshold
    kind: integer
    value: 0
    description: Include a positive value `n` in your config to allow for `target-postgres`
      to encounter at most `n` invalid records per stream before giving up.
  - name: disable_collection
    kind: boolean
    value: false
    description: 'Include `true` in your config to disable Singer Usage Logging: https://github.com/datamill-co/target-postgres#usage-logging'
  - name: logging_level
    kind: options
    value: INFO
    options:
    - label: Debug
      value: DEBUG
    - label: Info
      value: INFO
    - label: Warning
      value: WARNING
    - label: Error
      value: ERROR
    - label: Critical
      value: CRITICAL
    description: The level for logging. Set to `DEBUG` to get things like queries
      executed, timing of those queries, etc.
  - name: persist_empty_tables
    kind: boolean
    value: false
    description: Whether the Target should create tables which have no records present
      in Remote.
  - name: max_batch_rows
    kind: integer
    value: 200000
    description: The maximum number of rows to buffer in memory before writing to
      the destination table in Postgres
  - name: max_buffer_size
    kind: integer
    value: 104857600
    description: 'The maximum number of bytes to buffer in memory before writing to
      the destination table in Postgres. Default: 100MB in bytes'
  - name: batch_detection_threshold
    kind: integer
    description: How often, in rows received, to count the buffered rows and bytes
      to check if a flush is necessary. There's a slight performance penalty to checking
      the buffered records count or bytesize, so this controls how often this is polled
      in order to mitigate the penalty. This value is usually not necessary to set
      as the default is dynamically adjusted to check reasonably often.
  - name: state_support
    kind: boolean
    value: true
    description: Whether the Target should emit `STATE` messages to stdout for further
      consumption. In this mode, which is on by default, STATE messages are buffered
      in memory until all the records that occurred before them are flushed according
      to the batch flushing schedule the target is configured with.
  - name: add_upsert_indexes
    kind: boolean
    value: true
    description: Whether the Target should create column indexes on the important
      columns used during data loading. These indexes will make data loading slightly
      slower but the deduplication phase much faster. Defaults to on for better baseline
      performance.
  - name: before_run_sql
    description: Raw SQL statement(s) to execute as soon as the connection to Postgres
      is opened by the target. Useful for setup like `SET ROLE` or other connection
      state that is important.
  - name: after_run_sql
    description: Raw SQL statement(s) to execute as soon as the connection to Postgres
      is opened by the target. Useful for setup like `SET ROLE` or other connection
      state that is important.
- name: transferwise
  default: true
  docs: https://transferwise.github.io/pipelinewise/connectors/targets/postgres.html
  repo: https://github.com/transferwise/pipelinewise-target-postgres
  maintenance_status: active
  pip_url: pipelinewise-target-postgres
  settings_group_validation:
  - - host
    - port
    - user
    - password
    - dbname
    - default_target_schema
  settings:
  - name: host
    value: localhost
    description: PostgreSQL host
    label: Host
  - name: port
    kind: integer
    value: 5432
    description: PostgreSQL port
    label: Port
  - name: user
    description: PostgreSQL user
    label: User
  - name: password
    kind: password
    description: PostgreSQL password
    label: Password
  - name: dbname
    description: PostgreSQL database name
    label: Dbname
  - name: ssl
    kind: boolean
    value: false
    label: Ssl
  - name: default_target_schema
    description: Name of the schema where the tables will be created. If `schema_mapping`
      is not defined then every stream sent by the tap is loaded into this schema.
    label: Default Target Schema
  - name: batch_size_rows
    kind: integer
    value: 100000
    description: Maximum number of rows in each batch. At the end of each batch, the
      rows in the batch are loaded into Postgres.
    label: Batch Size Rows
  - name: flush_all_streams
    kind: boolean
    value: false
    description: 'Flush and load every stream into Postgres when one batch is full.
      Warning: This may trigger the COPY command to use files with low number of records.'
    label: Flush All Streams
  - name: parallelism
    kind: integer
    value: 0
    description: The number of threads used to flush tables. 0 will create a thread
      for each stream, up to parallelism_max. -1 will create a thread for each CPU
      core. Any other positive number will create that number of threads, up to parallelism_max.
    label: Parallelism
  - name: parallelism_max
    kind: integer
    value: 16
    description: Max number of parallel threads to use when flushing tables.
    label: Parallelism Max
  - name: default_target_schema_select_permission
    description: Grant USAGE privilege on newly created schemas and grant SELECT privilege
      on newly created tables to a specific role or a list of roles. If `schema_mapping`
      is not defined then every stream sent by the tap is granted accordingly.
    label: Default Target Schema Select Permission
  - name: schema_mapping
    kind: object
    description: 'Useful if you want to load multiple streams from one tap to multiple
      Postgres schemas. If the tap sends the `stream_id` in `<schema_name>-<table_name>`
      format then this option overwrites the `default_target_schema` value. Note,
      that using `schema_mapping` you can overwrite the `default_target_schema_select_permission`
      value to grant SELECT permissions to different groups per schemas or optionally
      you can create indices automatically for the replicated tables.

      '
    label: Schema Mapping
  - name: add_metadata_columns
    kind: boolean
    value: false
    description: Metadata columns add extra row level information about data ingestions,
      (i.e. when was the row read in source, when was inserted or deleted in postgres
      etc.) Metadata columns are creating automatically by adding extra columns to
      the tables with a column prefix `_SDC_`. The column names are following the
      stitch naming conventions documented at https://www.stitchdata.com/docs/data-structure/integration-schemas#sdc-columns.
      Enabling metadata columns will flag the deleted rows by setting the `_SDC_DELETED_AT`
      metadata column. Without the `add_metadata_columns` option the deleted rows
      from singer taps will not be recongisable in Postgres.
    label: Add Metadata Columns
  - name: hard_delete
    kind: boolean
    value: false
    description: When `hard_delete` option is true then DELETE SQL commands will be
      performed in Postgres to delete rows in tables. It's achieved by continuously
      checking the `_SDC_DELETED_AT` metadata column sent by the singer tap. Due to
      deleting rows requires metadata columns, `hard_delete` option automatically
      enables the `add_metadata_columns` option as well.
    label: Hard Delete
  - name: data_flattening_max_level
    kind: integer
    value: 0
    description: Object type RECORD items from taps can be transformed to flattened
      columns by creating columns automatically. When value is 0 (default) then flattening
      functionality is turned off.
    label: Data Flattening Max Level
  - name: primary_key_required
    kind: boolean
    value: true
    description: Log based and Incremental replications on tables with no Primary
      Key cause duplicates when merging UPDATE events. When set to true, stop loading
      data if no Primary Key is defined.
    label: Primary Key Required
  - name: validate_records
    kind: boolean
    value: false
    description: Validate every single record message to the corresponding JSON schema.
      This option is disabled by default and invalid RECORD messages will fail only
      at load time by Postgres. Enabling this option will detect invalid records earlier
      but could cause performance degradation.
    label: Validate Records
  - name: temp_dir
    description: '(Default: platform-dependent) Directory of temporary CSV files with
      RECORD messages.'
    label: Temp Dir
- name: meltano
  repo: https://github.com/meltano/target-postgres
  maintenance_status: active
  pip_url: git+https://github.com/meltano/target-postgres.git
  settings_group_validation:
  - - url
    - schema
  - - user
    - password
    - host
    - port
    - dbname
    - schema
  settings:
  - name: user
    value: warehouse
  - name: password
    kind: password
    value: warehouse
  - name: host
    value: localhost
  - name: port
    kind: integer
    value: 5502
  - name: dbname
    label: Database Name
    value: warehouse
  - name: url
    label: URL
    description: Lets you set `user`, `password`, `host`, `port`, and `dbname` in
      one go using a `postgresql://` URI. Takes precedence over the other settings
      when set.
  - name: schema
    schema_id: true
entity_url: https://www.postgresql.org/
entity_type: database
